import { ILLMService, Message, StreamHandlers, LLMResponse, ModelInfo, ModelOption, ToolDefinition, ToolCall } from './types';
import { ModelConfig } from '../model/types';
import { ModelManager } from '../model/manager';
import { APIError, RequestConfigError } from './errors';
import OpenAI from 'openai';
import { GoogleGenerativeAI, GenerativeModel } from '@google/generative-ai';
import { isVercel, isDocker, getProxyUrl, isRunningInElectron } from '../../utils/environment';
import { ElectronLLMProxy } from './electron-proxy';

/**
 * LLMæœåŠ¡å®ç° - åŸºäºå®˜æ–¹SDK
 */
export class LLMService implements ILLMService {
  constructor(private modelManager: ModelManager) { }

  /**
   * éªŒè¯æ¶ˆæ¯æ ¼å¼
   */
  private validateMessages(messages: Message[]): void {
    if (!Array.isArray(messages)) {
      throw new RequestConfigError('æ¶ˆæ¯å¿…é¡»æ˜¯æ•°ç»„æ ¼å¼');
    }
    if (messages.length === 0) {
      throw new RequestConfigError('æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º');
    }
    messages.forEach(msg => {
      if (!msg.role || !msg.content) {
        throw new RequestConfigError('æ¶ˆæ¯æ ¼å¼æ— æ•ˆ: ç¼ºå°‘å¿…è¦å­—æ®µ');
      }
      if (!['system', 'user', 'assistant'].includes(msg.role)) {
        throw new RequestConfigError(`ä¸æ”¯æŒçš„æ¶ˆæ¯ç±»å‹: ${msg.role}`);
      }
      if (typeof msg.content !== 'string') {
        throw new RequestConfigError('æ¶ˆæ¯å†…å®¹å¿…é¡»æ˜¯å­—ç¬¦ä¸²');
      }
    });
  }

  /**
   * éªŒè¯æ¨¡å‹é…ç½®
   */
  private validateModelConfig(modelConfig: ModelConfig): void {
    if (!modelConfig) {
      throw new RequestConfigError('æ¨¡å‹é…ç½®ä¸èƒ½ä¸ºç©º');
    }
    if (!modelConfig.provider) {
      throw new RequestConfigError('æ¨¡å‹æä¾›å•†ä¸èƒ½ä¸ºç©º');
    }
    // API keyå…è®¸ä¸ºç©ºå­—ç¬¦ä¸²ï¼ŒæŸäº›æœåŠ¡ï¼ˆå¦‚Ollamaï¼‰ä¸éœ€è¦API key
    if (!modelConfig.defaultModel) {
      throw new RequestConfigError('é»˜è®¤æ¨¡å‹ä¸èƒ½ä¸ºç©º');
    }
    if (!modelConfig.enabled) {
      throw new RequestConfigError('æ¨¡å‹æœªå¯ç”¨');
    }
  }

  /**
   * è·å–OpenAIå®ä¾‹
   */
  private getOpenAIInstance(modelConfig: ModelConfig, isStream: boolean = false): OpenAI {

    const apiKey = modelConfig.apiKey || '';

    // å¤„ç†baseURLï¼Œå¦‚æœä»¥'/chat/completions'ç»“å°¾åˆ™å»æ‰
    let processedBaseURL = modelConfig.baseURL;
    if (processedBaseURL?.endsWith('/chat/completions')) {
      processedBaseURL = processedBaseURL.slice(0, -'/chat/completions'.length);
    }

    // ä½¿ç”¨ä»£ç†å¤„ç†è·¨åŸŸé—®é¢˜
    let finalBaseURL = processedBaseURL;
    if (processedBaseURL) {
      if (modelConfig.useVercelProxy === true && isVercel()) {
        finalBaseURL = getProxyUrl(processedBaseURL, isStream);
        console.log(`ä½¿ç”¨Vercel${isStream ? 'æµå¼' : ''}APIä»£ç†:`, finalBaseURL);
      } else if (modelConfig.useDockerProxy === true && isDocker()) {
        finalBaseURL = getProxyUrl(processedBaseURL, isStream);
        console.log(`ä½¿ç”¨Docker${isStream ? 'æµå¼' : ''}APIä»£ç†:`, finalBaseURL);
      }
    }

    // åˆ›å»ºOpenAIå®ä¾‹é…ç½®
    const defaultTimeout = isStream ? 90000 : 60000;
    const timeout = modelConfig.llmParams?.timeout !== undefined
                    ? modelConfig.llmParams.timeout
                    : defaultTimeout;
    
    const config: any = {
      apiKey: apiKey,
      baseURL: finalBaseURL,
      timeout: timeout,
      maxRetries: isStream ? 2 : 3
    };

    // In any browser-like environment, we must set this flag to true 
    // to bypass the SDK's environment check.
    if (typeof window !== 'undefined') {
      config.dangerouslyAllowBrowser = true;
      console.log('[LLM Service] Browser-like environment detected. Setting dangerouslyAllowBrowser=true.');
    }

    const instance = new OpenAI(config);

    return instance;
  }

  /**
   * è·å–Geminiå®ä¾‹
   */
  private getGeminiModel(modelConfig: ModelConfig, systemInstruction?: string, isStream: boolean = false): GenerativeModel {
    const apiKey = modelConfig.apiKey || '';

    // åˆ›å»ºGoogleGenerativeAIå®ä¾‹ - æ—§ç‰ˆæœ¬ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²API key
    const genAI = new GoogleGenerativeAI(apiKey);

    // åˆ›å»ºæ¨¡å‹é…ç½®
    const modelOptions: any = {
      model: modelConfig.defaultModel
    };

    // å¦‚æœæœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œæ·»åŠ åˆ°æ¨¡å‹é…ç½®ä¸­
    if (systemInstruction) {
      modelOptions.systemInstruction = systemInstruction;
    }

    // å¤„ç†baseURLï¼Œå¦‚æœä»¥'/v1beta'ç»“å°¾åˆ™å»æ‰
    let processedBaseURL = modelConfig.baseURL;
    if (processedBaseURL?.endsWith('/v1beta')) {
      processedBaseURL = processedBaseURL.slice(0, -'/v1beta'.length);
    }
    // ä½¿ç”¨ä»£ç†å¤„ç†è·¨åŸŸé—®é¢˜
    let finalBaseURL = processedBaseURL;
    if (processedBaseURL) {
      if (modelConfig.useVercelProxy === true && isVercel()) {
        finalBaseURL = getProxyUrl(processedBaseURL, isStream);
        console.log(`ä½¿ç”¨Vercel${isStream ? 'æµå¼' : ''}APIä»£ç†:`, finalBaseURL);
      } else if (modelConfig.useDockerProxy === true && isDocker()) {
        finalBaseURL = getProxyUrl(processedBaseURL, isStream);
        console.log(`ä½¿ç”¨Docker${isStream ? 'æµå¼' : ''}APIä»£ç†:`, finalBaseURL);
      }
    }
    return genAI.getGenerativeModel(modelOptions, { "baseUrl": finalBaseURL });
  }

  /**
   * å‘é€OpenAIæ¶ˆæ¯ï¼ˆç»“æ„åŒ–æ ¼å¼ï¼‰
   */
  private async sendOpenAIMessageStructured(messages: Message[], modelConfig: ModelConfig): Promise<LLMResponse> {
    const openai = this.getOpenAIInstance(modelConfig);

    const formattedMessages = messages.map(msg => ({
      role: msg.role,
      content: msg.content
    }));

    const {
      timeout, // Handled in getOpenAIInstance
      model: llmParamsModel, // Avoid overriding main model
      messages: llmParamsMessages, // Avoid overriding main messages
      ...restLlmParams
    } = modelConfig.llmParams || {};

    const completionConfig: any = {
      model: modelConfig.defaultModel,
      messages: formattedMessages,
      ...restLlmParams // Spread other params from llmParams
    };

    try {
      const response = await openai.chat.completions.create(completionConfig);

      // å¤„ç†å“åº”ä¸­çš„ reasoning_content å’Œæ™®é€š content
      const choice = response.choices[0];
      if (!choice?.message) {
        throw new Error('æœªæ”¶åˆ°æœ‰æ•ˆçš„å“åº”');
      }

      let content = choice.message.content || '';
      let reasoning = '';

      // å¤„ç†æ¨ç†å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
      // SiliconFlow ç­‰æä¾›å•†åœ¨ choice.message ä¸­å¹¶åˆ—æä¾› reasoning_content å­—æ®µ
      if ((choice.message as any).reasoning_content) {
        reasoning = (choice.message as any).reasoning_content;
      } else {
        // æ£€æµ‹å¹¶åˆ†ç¦»contentä¸­çš„thinkæ ‡ç­¾
        const thinkMatch = content.match(/<think>(.*?)<\/think>/s);
        if (thinkMatch) {
          reasoning = thinkMatch[1];
          content = content.replace(/<think>.*?<\/think>/s, '').trim();
        }
      }

      const result: LLMResponse = {
        content: content,
        reasoning: reasoning || undefined,
        metadata: {
          model: modelConfig.defaultModel,
          finishReason: choice.finish_reason || undefined
        }
      };

      return result;
    } catch (error) {
      console.error('OpenAI APIè°ƒç”¨å¤±è´¥:', error);
      throw new Error(`OpenAI APIè°ƒç”¨å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`);
    }
  }



  /**
   * å‘é€Geminiæ¶ˆæ¯ï¼ˆç»“æ„åŒ–æ ¼å¼ï¼‰
   */
  private async sendGeminiMessageStructured(messages: Message[], modelConfig: ModelConfig): Promise<LLMResponse> {
    // æå–ç³»ç»Ÿæ¶ˆæ¯
    const systemMessages = messages.filter(msg => msg.role === 'system');
    const systemInstruction = systemMessages.length > 0
      ? systemMessages.map(msg => msg.content).join('\n')
      : '';

    // è·å–å¸¦æœ‰ç³»ç»ŸæŒ‡ä»¤çš„æ¨¡å‹å®ä¾‹
    const model = this.getGeminiModel(modelConfig, systemInstruction, false);

    // è¿‡æ»¤å‡ºç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
    const conversationMessages = messages.filter(msg => msg.role !== 'system');

    // åˆ›å»ºèŠå¤©ä¼šè¯
    const generationConfig = this.buildGeminiGenerationConfig(modelConfig.llmParams);

    const chatOptions: any = {
      history: this.formatGeminiHistory(conversationMessages)
    };
    if (Object.keys(generationConfig).length > 0) {
      chatOptions.generationConfig = generationConfig;
    }
    const chat = model.startChat(chatOptions);

    // è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    const lastUserMessage = conversationMessages.length > 0 &&
      conversationMessages[conversationMessages.length - 1].role === 'user'
      ? conversationMessages[conversationMessages.length - 1].content
      : '';

    // å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œè¿”å›ç©ºå“åº”
    if (!lastUserMessage) {
      return {
        content: '',
        metadata: {
          model: modelConfig.defaultModel
        }
      };
    }

    // å‘é€æ¶ˆæ¯å¹¶è·å–å“åº”
    const result = await chat.sendMessage(lastUserMessage);
    
    return {
      content: result.response.text(),
      metadata: {
        model: modelConfig.defaultModel
      }
    };
  }



  /**
   * æ ¼å¼åŒ–Geminiå†å²æ¶ˆæ¯
   */
  private formatGeminiHistory(messages: Message[]): any[] {
    if (messages.length <= 1) {
      return [];
    }

    // æ’é™¤æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆå°†ç”±sendMessageå•ç‹¬å‘é€ï¼‰
    const historyMessages = messages.slice(0, -1);
    const formattedHistory = [];

    for (let i = 0; i < historyMessages.length; i++) {
      const msg = historyMessages[i];
      if (msg.role === 'user') {
        formattedHistory.push({
          role: 'user',
          parts: [{ text: msg.content }]
        });
      } else if (msg.role === 'assistant') {
        formattedHistory.push({
          role: 'model',
          parts: [{ text: msg.content }]
        });
      }
    }

    return formattedHistory;
  }

  /**
   * å‘é€æ¶ˆæ¯ï¼ˆç»“æ„åŒ–æ ¼å¼ï¼‰
   */
  async sendMessageStructured(messages: Message[], provider: string): Promise<LLMResponse> {
    try {
      if (!provider) {
        throw new RequestConfigError('æ¨¡å‹æä¾›å•†ä¸èƒ½ä¸ºç©º');
      }

      const modelConfig = await this.modelManager.getModel(provider);
      if (!modelConfig) {
        throw new RequestConfigError(`æ¨¡å‹ ${provider} ä¸å­˜åœ¨`);
      }

      this.validateModelConfig(modelConfig);
      this.validateMessages(messages);

      console.log('å‘é€æ¶ˆæ¯:', {
        provider: modelConfig.provider,
        model: modelConfig.defaultModel,
        messagesCount: messages.length
      });

      if (modelConfig.provider === 'gemini') {
        return this.sendGeminiMessageStructured(messages, modelConfig);
      } else {
        // OpenAIå…¼å®¹æ ¼å¼çš„APIï¼ŒåŒ…æ‹¬DeepSeekå’Œè‡ªå®šä¹‰æ¨¡å‹
        return this.sendOpenAIMessageStructured(messages, modelConfig);
      }
    } catch (error: any) {
      if (error instanceof RequestConfigError || error instanceof APIError) {
        throw error;
      }
      throw new APIError(`å‘é€æ¶ˆæ¯å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * å‘é€æ¶ˆæ¯ï¼ˆä¼ ç»Ÿæ ¼å¼ï¼Œåªè¿”å›ä¸»è¦å†…å®¹ï¼‰
   */
  async sendMessage(messages: Message[], provider: string): Promise<string> {
    const response = await this.sendMessageStructured(messages, provider);
    
    // åªè¿”å›ä¸»è¦å†…å®¹ï¼Œä¸åŒ…å«æ¨ç†å†…å®¹
    // å¦‚æœéœ€è¦æ¨ç†å†…å®¹ï¼Œè¯·ä½¿ç”¨ sendMessageStructured æ–¹æ³•
    return response.content;
  }

  /**
   * å‘é€æ¶ˆæ¯ï¼ˆæµå¼ï¼Œæ”¯æŒç»“æ„åŒ–å’Œä¼ ç»Ÿæ ¼å¼ï¼‰
   */
  async sendMessageStream(
    messages: Message[],
    provider: string,
    callbacks: StreamHandlers
  ): Promise<void> {
    try {
      console.log('å¼€å§‹æµå¼è¯·æ±‚:', { provider, messagesCount: messages.length });
      this.validateMessages(messages);

      const modelConfig = await this.modelManager.getModel(provider);
      if (!modelConfig) {
        throw new RequestConfigError(`æ¨¡å‹ ${provider} ä¸å­˜åœ¨`);
      }

      this.validateModelConfig(modelConfig);

      console.log('è·å–åˆ°æ¨¡å‹å®ä¾‹:', {
        provider: modelConfig.provider,
        model: modelConfig.defaultModel
      });

      if (modelConfig.provider === 'gemini') {
        await this.streamGeminiMessage(messages, modelConfig, callbacks);
      } else {
        // OpenAIå…¼å®¹æ ¼å¼çš„APIï¼ŒåŒ…æ‹¬DeepSeekå’Œè‡ªå®šä¹‰æ¨¡å‹
        await this.streamOpenAIMessage(messages, modelConfig, callbacks);
      }
    } catch (error) {
      console.error('æµå¼è¯·æ±‚å¤±è´¥:', error);
      callbacks.onError(error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  /**
   * å‘é€æ¶ˆæ¯ï¼ˆæµå¼ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
   * ğŸ†• æ”¯æŒå·¥å…·è°ƒç”¨çš„æµå¼æ¶ˆæ¯å‘é€
   */
  async sendMessageStreamWithTools(
    messages: Message[],
    provider: string,
    tools: ToolDefinition[],
    callbacks: StreamHandlers
  ): Promise<void> {
    try {
      console.log('å¼€å§‹å¸¦å·¥å…·çš„æµå¼è¯·æ±‚:', { 
        provider, 
        messagesCount: messages.length,
        toolsCount: tools.length 
      });
      
      this.validateMessages(messages);

      const modelConfig = await this.modelManager.getModel(provider);
      if (!modelConfig) {
        throw new RequestConfigError(`æ¨¡å‹ ${provider} ä¸å­˜åœ¨`);
      }

      this.validateModelConfig(modelConfig);

      console.log('è·å–åˆ°æ¨¡å‹å®ä¾‹ï¼ˆå¸¦å·¥å…·ï¼‰:', {
        provider: modelConfig.provider,
        model: modelConfig.defaultModel,
        tools: tools.map(t => t.function.name)
      });

      if (modelConfig.provider === 'gemini') {
        // Geminiå·¥å…·è°ƒç”¨æ”¯æŒ
        await this.streamGeminiMessageWithTools(messages, modelConfig, tools, callbacks);
      } else {
        // OpenAIå…¼å®¹æ ¼å¼çš„APIå·¥å…·è°ƒç”¨
        await this.streamOpenAIMessageWithTools(messages, modelConfig, tools, callbacks);
      }
    } catch (error) {
      console.error('å¸¦å·¥å…·çš„æµå¼è¯·æ±‚å¤±è´¥:', error);
      callbacks.onError(error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  /**
   * å¤„ç†æµå¼å†…å®¹ä¸­çš„thinkæ ‡ç­¾(ç”¨äºæµå¼åœºæ™¯)
   */
  private processStreamContentWithThinkTags(
    content: string, 
    callbacks: StreamHandlers,
    thinkState: { isInThinkMode: boolean; buffer: string }
  ): void {
    // å¦‚æœæ²¡æœ‰æ¨ç†å›è°ƒï¼Œç›´æ¥å‘é€åˆ°ä¸»è¦å†…å®¹æµ
    if (!callbacks.onReasoningToken) {
      callbacks.onToken(content);
      return;
    }

    // å°†æ–°å†…å®¹æ·»åŠ åˆ°ç¼“å†²åŒº
    thinkState.buffer += content;
    let remaining = thinkState.buffer;
    let processed = '';
    
    while (remaining.length > 0) {
      if (!thinkState.isInThinkMode) {
        // ä¸åœ¨thinkæ¨¡å¼ä¸­ï¼ŒæŸ¥æ‰¾<think>æ ‡ç­¾
        const thinkStartIndex = remaining.indexOf('<think>');
        
        if (thinkStartIndex !== -1) {
          // æ‰¾åˆ°äº†å¼€å§‹æ ‡ç­¾
          // å‘é€å¼€å§‹æ ‡ç­¾å‰çš„å†…å®¹åˆ°ä¸»è¦æµ
          if (thinkStartIndex > 0) {
            const beforeThink = remaining.slice(0, thinkStartIndex);
            callbacks.onToken(beforeThink);
            processed += beforeThink + '<think>';
          } else {
            processed += '<think>';
          }
          
          // è¿›å…¥thinkæ¨¡å¼
          thinkState.isInThinkMode = true;
          remaining = remaining.slice(thinkStartIndex + 7); // 7 = '<think>'.length
        } else {
          // æ²¡æœ‰æ‰¾åˆ°å¼€å§‹æ ‡ç­¾
          // æ£€æŸ¥bufferæœ«å°¾æ˜¯å¦å¯èƒ½æ˜¯ä¸å®Œæ•´çš„æ ‡ç­¾å¼€å§‹
          if (remaining.endsWith('<') || remaining.endsWith('<t') || 
              remaining.endsWith('<th') || remaining.endsWith('<thi') || 
              remaining.endsWith('<thin') || remaining.endsWith('<think')) {
            // å¯èƒ½æ˜¯ä¸å®Œæ•´çš„æ ‡ç­¾ï¼Œä¿ç•™åœ¨bufferä¸­ç­‰å¾…æ›´å¤šå†…å®¹
            thinkState.buffer = remaining;
            return;
          } else {
            // ç¡®å®šæ²¡æœ‰æ ‡ç­¾ï¼Œå‘é€æ‰€æœ‰å†…å®¹åˆ°ä¸»è¦æµ
            callbacks.onToken(remaining);
            processed += remaining;
            remaining = '';
          }
        }
      } else {
        // åœ¨thinkæ¨¡å¼ä¸­ï¼ŒæŸ¥æ‰¾</think>æ ‡ç­¾
        const thinkEndIndex = remaining.indexOf('</think>');
        
        if (thinkEndIndex !== -1) {
          // æ‰¾åˆ°äº†ç»“æŸæ ‡ç­¾
          // å‘é€ç»“æŸæ ‡ç­¾å‰çš„å†…å®¹åˆ°æ¨ç†æµ
          if (thinkEndIndex > 0) {
            const thinkContent = remaining.slice(0, thinkEndIndex);
            callbacks.onReasoningToken(thinkContent);
          }
          
          // é€€å‡ºthinkæ¨¡å¼
          thinkState.isInThinkMode = false;
          processed += remaining.slice(0, thinkEndIndex) + '</think>';
          remaining = remaining.slice(thinkEndIndex + 8); // 8 = '</think>'.length
        } else {
          // æ²¡æœ‰æ‰¾åˆ°ç»“æŸæ ‡ç­¾
          // æ£€æŸ¥bufferæœ«å°¾æ˜¯å¦å¯èƒ½æ˜¯ä¸å®Œæ•´çš„ç»“æŸæ ‡ç­¾
          if (remaining.endsWith('<') || remaining.endsWith('</') || 
              remaining.endsWith('</t') || remaining.endsWith('</th') || 
              remaining.endsWith('</thi') || remaining.endsWith('</thin') || 
              remaining.endsWith('</think')) {
            // å¯èƒ½æ˜¯ä¸å®Œæ•´çš„ç»“æŸæ ‡ç­¾ï¼Œä¿ç•™åœ¨bufferä¸­ç­‰å¾…æ›´å¤šå†…å®¹
            thinkState.buffer = remaining;
            return;
          } else {
            // ç¡®å®šæ˜¯thinkå†…å®¹ï¼Œå‘é€åˆ°æ¨ç†æµ
            callbacks.onReasoningToken(remaining);
            processed += remaining;
            remaining = '';
          }
        }
      }
    }
    
    // æ›´æ–°ç¼“å†²åŒºä¸ºå·²å¤„ç†çš„å†…å®¹
    thinkState.buffer = '';
  }

  /**
   * æµå¼å‘é€OpenAIæ¶ˆæ¯
   */
  private async streamOpenAIMessage(
    messages: Message[],
    modelConfig: ModelConfig,
    callbacks: StreamHandlers
  ): Promise<void> {
    try {
      // è·å–æµå¼OpenAIå®ä¾‹
      const openai = this.getOpenAIInstance(modelConfig, true);

      const formattedMessages = messages.map(msg => ({
        role: msg.role,
        content: msg.content
      }));

      console.log('å¼€å§‹åˆ›å»ºæµå¼è¯·æ±‚...');
      const {
        timeout, // Handled in getOpenAIInstance
        model: llmParamsModel, // Avoid overriding main model
        messages: llmParamsMessages, // Avoid overriding main messages
        stream: llmParamsStream, // Avoid overriding main stream flag
        ...restLlmParams
      } = modelConfig.llmParams || {};

      const completionConfig: any = {
        model: modelConfig.defaultModel,
        messages: formattedMessages,
        stream: true, // Essential for streaming
        ...restLlmParams // User-defined parameters from llmParams
      };
      
      // ç›´æ¥ä½¿ç”¨æµå¼å“åº”ï¼Œæ— éœ€ç±»å‹è½¬æ¢
      const stream = await openai.chat.completions.create(completionConfig);

      console.log('æˆåŠŸè·å–åˆ°æµå¼å“åº”');

      // ä½¿ç”¨ç±»å‹æ–­è¨€æ¥ç¡®ä¿TypeScriptçŸ¥é“è¿™æ˜¯æµå¼å“åº”
      let accumulatedReasoning = '';
      let accumulatedContent = '';
      
      // thinkæ ‡ç­¾çŠ¶æ€è·Ÿè¸ª
      const thinkState = { isInThinkMode: false, buffer: '' };

      for await (const chunk of stream as any) {
        // å¤„ç†æ¨ç†å†…å®¹ï¼ˆSiliconFlow ç­‰æä¾›å•†åœ¨ delta ä¸­æä¾› reasoning_contentï¼‰
        const reasoningContent = chunk.choices[0]?.delta?.reasoning_content || '';
        if (reasoningContent) {
          accumulatedReasoning += reasoningContent;
          
          // å¦‚æœæœ‰æ¨ç†å›è°ƒï¼Œå‘é€æ¨ç†å†…å®¹
          if (callbacks.onReasoningToken) {
            callbacks.onReasoningToken(reasoningContent);
          }
          await new Promise(resolve => setTimeout(resolve, 10));
        }

        // å¤„ç†ä¸»è¦å†…å®¹
        const content = chunk.choices[0]?.delta?.content || '';
        if (content) {
          accumulatedContent += content;
          
          // ä½¿ç”¨æµå¼thinkæ ‡ç­¾å¤„ç†
          this.processStreamContentWithThinkTags(content, callbacks, thinkState);
          
          await new Promise(resolve => setTimeout(resolve, 10));
        }
      }

      console.log('æµå¼å“åº”å®Œæˆ');
      
      // æ„å»ºå®Œæ•´å“åº”
      const response: LLMResponse = {
        content: accumulatedContent,
        reasoning: accumulatedReasoning || undefined,
        metadata: {
          model: modelConfig.defaultModel
        }
      };

      callbacks.onComplete(response);
    } catch (error) {
      console.error('æµå¼å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™:', error);
      callbacks.onError(error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  /**
   * æµå¼å‘é€OpenAIæ¶ˆæ¯ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
   * ğŸ†• åŸºäºstreamOpenAIMessageæ‰©å±•å·¥å…·è°ƒç”¨æ”¯æŒ
   */
  private async streamOpenAIMessageWithTools(
    messages: Message[],
    modelConfig: ModelConfig,
    tools: ToolDefinition[],
    callbacks: StreamHandlers
  ): Promise<void> {
    try {
      // è·å–æµå¼OpenAIå®ä¾‹
      const openai = this.getOpenAIInstance(modelConfig, true);

      const formattedMessages = messages.map(msg => ({
        role: msg.role,
        content: msg.content
      }));

      console.log('å¼€å§‹åˆ›å»ºå¸¦å·¥å…·çš„æµå¼è¯·æ±‚...');
      const {
        timeout,
        model: llmParamsModel,
        messages: llmParamsMessages,
        stream: llmParamsStream,
        tools: llmParamsTools,
        ...restLlmParams
      } = modelConfig.llmParams || {};

      const completionConfig: any = {
        model: modelConfig.defaultModel,
        messages: formattedMessages,
        tools: tools,
        tool_choice: 'auto',
        stream: true,
        ...restLlmParams
      };
      
      const stream = await openai.chat.completions.create(completionConfig);
      console.log('æˆåŠŸè·å–åˆ°å¸¦å·¥å…·çš„æµå¼å“åº”');

      let accumulatedReasoning = '';
      let accumulatedContent = '';
      const toolCalls: any[] = [];
      const thinkState = { isInThinkMode: false, buffer: '' };

      for await (const chunk of stream as any) {
        // å¤„ç†æ¨ç†å†…å®¹
        const reasoningContent = chunk.choices[0]?.delta?.reasoning_content || '';
        if (reasoningContent) {
          accumulatedReasoning += reasoningContent;
          if (callbacks.onReasoningToken) {
            callbacks.onReasoningToken(reasoningContent);
          }
          await new Promise(resolve => setTimeout(resolve, 10));
        }

        // ğŸ†• å¤„ç†å·¥å…·è°ƒç”¨
        const toolCallDeltas = chunk.choices[0]?.delta?.tool_calls;
        if (toolCallDeltas) {
          for (const toolCallDelta of toolCallDeltas) {
            if (toolCallDelta.index !== undefined) {
              while (toolCalls.length <= toolCallDelta.index) {
                toolCalls.push({ id: '', type: 'function' as const, function: { name: '', arguments: '' } });
              }
              
              const currentToolCall = toolCalls[toolCallDelta.index];
              
              if (toolCallDelta.id) currentToolCall.id = toolCallDelta.id;
              if (toolCallDelta.type) currentToolCall.type = toolCallDelta.type;
              if (toolCallDelta.function) {
                if (toolCallDelta.function.name) {
                  currentToolCall.function.name += toolCallDelta.function.name;
                }
                if (toolCallDelta.function.arguments) {
                  currentToolCall.function.arguments += toolCallDelta.function.arguments;
                }
                
                // å½“å·¥å…·è°ƒç”¨å®Œæ•´æ—¶ï¼Œé€šçŸ¥å›è°ƒ
                if (currentToolCall.id && currentToolCall.function.name && 
                    toolCallDelta.function.arguments && callbacks.onToolCall) {
                  try {
                    JSON.parse(currentToolCall.function.arguments);
                    callbacks.onToolCall(currentToolCall);
                  } catch {
                    // JSON è¿˜ä¸å®Œæ•´
                  }
                }
              }
            }
          }
        }

        // å¤„ç†ä¸»è¦å†…å®¹
        const content = chunk.choices[0]?.delta?.content || '';
        if (content) {
          accumulatedContent += content;
          this.processStreamContentWithThinkTags(content, callbacks, thinkState);
          await new Promise(resolve => setTimeout(resolve, 10));
        }
      }

      console.log('å¸¦å·¥å…·çš„æµå¼å“åº”å®Œæˆ, å·¥å…·è°ƒç”¨æ•°é‡:', toolCalls.length);
      
      const response: LLMResponse = {
        content: accumulatedContent,
        reasoning: accumulatedReasoning || undefined,
        toolCalls: toolCalls.length > 0 ? toolCalls : undefined,
        metadata: { model: modelConfig.defaultModel }
      };

      callbacks.onComplete(response);
    } catch (error) {
      console.error('å¸¦å·¥å…·çš„æµå¼å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™:', error);
      callbacks.onError(error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  /**
   * æµå¼å‘é€Geminiæ¶ˆæ¯
   */
  private async streamGeminiMessage(
    messages: Message[],
    modelConfig: ModelConfig,
    callbacks: StreamHandlers
  ): Promise<void> {
    // æå–ç³»ç»Ÿæ¶ˆæ¯
    const systemMessages = messages.filter(msg => msg.role === 'system');
    const systemInstruction = systemMessages.length > 0
      ? systemMessages.map(msg => msg.content).join('\n')
      : '';

    // è·å–å¸¦æœ‰ç³»ç»ŸæŒ‡ä»¤çš„æ¨¡å‹å®ä¾‹
    const model = this.getGeminiModel(modelConfig, systemInstruction, true);

    // è¿‡æ»¤å‡ºç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
    const conversationMessages = messages.filter(msg => msg.role !== 'system');

    // åˆ›å»ºèŠå¤©ä¼šè¯
    const generationConfig = this.buildGeminiGenerationConfig(modelConfig.llmParams);

    const chatOptions: any = {
      history: this.formatGeminiHistory(conversationMessages)
    };
    if (Object.keys(generationConfig).length > 0) {
      chatOptions.generationConfig = generationConfig;
    }
    const chat = model.startChat(chatOptions);

    // è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    const lastUserMessage = conversationMessages.length > 0 &&
      conversationMessages[conversationMessages.length - 1].role === 'user'
      ? conversationMessages[conversationMessages.length - 1].content
      : '';

    // å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œå‘é€ç©ºå“åº”
    if (!lastUserMessage) {
      const response: LLMResponse = {
        content: '',
        metadata: {
          model: modelConfig.defaultModel
        }
      };
      
      callbacks.onComplete(response);
      return;
    }

    try {
      console.log('å¼€å§‹åˆ›å»ºGeminiæµå¼è¯·æ±‚...');
      const result = await chat.sendMessageStream(lastUserMessage);

      console.log('æˆåŠŸè·å–åˆ°æµå¼å“åº”');
      
      let accumulatedContent = '';

      for await (const chunk of result.stream) {
        const text = chunk.text();
        if (text) {
          accumulatedContent += text;
          callbacks.onToken(text);
          // æ·»åŠ å°å»¶è¿Ÿï¼Œè®©UIæœ‰æ—¶é—´æ›´æ–°
          await new Promise(resolve => setTimeout(resolve, 10));
        }
      }

      console.log('æµå¼å“åº”å®Œæˆ');
      
      // æ„å»ºå®Œæ•´å“åº”
      const response: LLMResponse = {
        content: accumulatedContent,
        metadata: {
          model: modelConfig.defaultModel
        }
      };

      callbacks.onComplete(response);
    } catch (error) {
      console.error('æµå¼å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™:', error);
      callbacks.onError(error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  /**
   * æµå¼å‘é€Geminiæ¶ˆæ¯ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
   * ğŸ†• åŸºäºstreamGeminiMessageæ‰©å±•å·¥å…·è°ƒç”¨æ”¯æŒ
   */
  private async streamGeminiMessageWithTools(
    messages: Message[],
    modelConfig: ModelConfig,
    tools: ToolDefinition[],
    callbacks: StreamHandlers
  ): Promise<void> {
    // æå–ç³»ç»Ÿæ¶ˆæ¯
    const systemMessages = messages.filter(msg => msg.role === 'system');
    const systemInstruction = systemMessages.length > 0
      ? systemMessages.map(msg => msg.content).join('\n')
      : '';

    // è·å–å¸¦æœ‰ç³»ç»ŸæŒ‡ä»¤çš„æ¨¡å‹å®ä¾‹
    const model = this.getGeminiModel(modelConfig, systemInstruction, true);

    // è¿‡æ»¤å‡ºç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
    const conversationMessages = messages.filter(msg => msg.role !== 'system');

    // è½¬æ¢å·¥å…·å®šä¹‰ä¸ºGeminiæ ¼å¼
    const geminiTools = this.convertToGeminiTools(tools);

    // åˆ›å»ºèŠå¤©ä¼šè¯
    const generationConfig = this.buildGeminiGenerationConfig(modelConfig.llmParams);

    const chatOptions: any = {
      history: this.formatGeminiHistory(conversationMessages),
      tools: geminiTools
    };
    if (Object.keys(generationConfig).length > 0) {
      chatOptions.generationConfig = generationConfig;
    }
    const chat = model.startChat(chatOptions);

    // è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    const lastUserMessage = conversationMessages.length > 0 &&
      conversationMessages[conversationMessages.length - 1].role === 'user'
      ? conversationMessages[conversationMessages.length - 1].content
      : '';

    // å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œå‘é€ç©ºå“åº”
    if (!lastUserMessage) {
      const response: LLMResponse = {
        content: '',
        metadata: {
          model: modelConfig.defaultModel
        }
      };
      
      callbacks.onComplete(response);
      return;
    }

    try {
      console.log('å¼€å§‹åˆ›å»ºGeminiå¸¦å·¥å…·çš„æµå¼è¯·æ±‚...', {
        toolsCount: tools.length,
        geminiTools: geminiTools
      });
      const result = await chat.sendMessageStream(lastUserMessage);

      console.log('æˆåŠŸè·å–åˆ°Geminiå¸¦å·¥å…·çš„æµå¼å“åº”');
      
      let accumulatedContent = '';
      const toolCalls: any[] = [];

      for await (const chunk of result.stream) {
        const text = chunk.text();
        if (text) {
          accumulatedContent += text;
          callbacks.onToken(text);
          // æ·»åŠ å°å»¶è¿Ÿï¼Œè®©UIæœ‰æ—¶é—´æ›´æ–°
          await new Promise(resolve => setTimeout(resolve, 10));
        }

        // å¤„ç†å·¥å…·è°ƒç”¨
        const functionCalls = chunk.functionCalls();
        if (functionCalls && functionCalls.length > 0) {
          for (const functionCall of functionCalls) {
            const toolCall: ToolCall = {
              id: `call_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
              type: 'function' as const,
              function: {
                name: functionCall.name,
                arguments: JSON.stringify(functionCall.args)
              }
            };
            
            toolCalls.push(toolCall);
            
            console.log('[Gemini] Tool call received:', toolCall);
            if (callbacks.onToolCall) {
              callbacks.onToolCall(toolCall);
            }
          }
        }
      }

      console.log('Geminiå¸¦å·¥å…·çš„æµå¼å“åº”å®Œæˆ, å·¥å…·è°ƒç”¨æ•°é‡:', toolCalls.length);
      
      // æ„å»ºå®Œæ•´å“åº”
      const response: LLMResponse = {
        content: accumulatedContent,
        toolCalls: toolCalls.length > 0 ? toolCalls : undefined,
        metadata: {
          model: modelConfig.defaultModel
        }
      };

      callbacks.onComplete(response);
    } catch (error) {
      console.error('Geminiå¸¦å·¥å…·çš„æµå¼å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™:', error);
      callbacks.onError(error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  /**
   * è½¬æ¢å·¥å…·å®šä¹‰ä¸ºGeminiæ ¼å¼
   */
  private convertToGeminiTools(tools: ToolDefinition[]): any[] {
    return [{
      functionDeclarations: tools.map(tool => ({
        name: tool.function.name,
        description: tool.function.description,
        parameters: tool.function.parameters
      }))
    }];
  }

  /**
   * æµ‹è¯•è¿æ¥
   */
  async testConnection(provider: string): Promise<void> {
    try {
      if (!provider) {
        throw new RequestConfigError('æ¨¡å‹æä¾›å•†ä¸èƒ½ä¸ºç©º');
      }
      console.log('æµ‹è¯•è¿æ¥provider:', {
        provider: provider,
      });

      const modelConfig = await this.modelManager.getModel(provider);
      if (!modelConfig) {
        throw new RequestConfigError(`æ¨¡å‹ ${provider} ä¸å­˜åœ¨`);
      }

      this.validateModelConfig(modelConfig);

      // å¯¹äº Ollamaï¼Œç›´æ¥æµ‹è¯• models ç«¯ç‚¹ï¼ˆæ›´å¿«ä¸”æ›´å¯é ï¼‰
      if (provider === 'ollama' || modelConfig.provider === 'ollama') {
        await this.testOllamaConnection(modelConfig);
        return;
      }

      // å¯¹äºå…¶ä»–æä¾›å•†ï¼Œå‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¶ˆæ¯
      const testMessages: Message[] = [
        {
          role: 'user',
          content: 'è¯·å›ç­”ok'
        }
      ];

      // ä½¿ç”¨ sendMessage è¿›è¡Œæµ‹è¯•
      await this.sendMessage(testMessages, provider);

    } catch (error: any) {
      if (error instanceof RequestConfigError || error instanceof APIError) {
        throw error;
      }
      throw new APIError(`è¿æ¥æµ‹è¯•å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * æµ‹è¯• Ollama è¿æ¥
   * ä½¿ç”¨ /v1/models ç«¯ç‚¹è¿›è¡Œå¿«é€Ÿè¿æ¥æµ‹è¯•
   */
  private async testOllamaConnection(modelConfig: ModelConfig): Promise<void> {
    try {
      const baseURL = modelConfig.baseURL?.replace('/v1', '') || 'http://localhost:11434';
      const testURL = `${baseURL}/v1/models`;

      console.log('æµ‹è¯• Ollama è¿æ¥:', testURL);

      // è·å–å½“å‰æµè§ˆå™¨è®¿é—®åœ°å€
      const currentOrigin = typeof window !== 'undefined' ? window.location.origin : 'unknown';
      console.log('æµè§ˆå™¨å½“å‰è®¿é—®åœ°å€:', currentOrigin);

      // æµ‹è¯• OpenAI å…¼å®¹çš„ /v1/models ç«¯ç‚¹
      const response = await fetch(testURL, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
        // æ·»åŠ è¶…æ—¶æ§åˆ¶
        signal: AbortSignal.timeout(10000), // 10ç§’è¶…æ—¶
      });

      if (!response.ok) {
        // 403 é€šå¸¸è¡¨ç¤º CORS é—®é¢˜
        if (response.status === 403) {
          throw new Error(`CORS é…ç½®é”™è¯¯: Ollama æ‹’ç»æ¥è‡ª ${currentOrigin} çš„è¯·æ±‚ã€‚è¯·åœ¨ Ollama æ‰€åœ¨æœºå™¨ä¸Šè®¾ç½®ç¯å¢ƒå˜é‡ OLLAMA_ORIGINS="${currentOrigin}" æˆ– OLLAMA_ORIGINS="*" åé‡å¯ Ollama æœåŠ¡ã€‚`);
        }
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data = await response.json();

      // éªŒè¯å“åº”æ ¼å¼
      if (!data || !data.data || !Array.isArray(data.data)) {
        throw new Error('Invalid response format from Ollama');
      }

      console.log('Ollama è¿æ¥æµ‹è¯•æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹:', data.data.length);
    } catch (error: any) {
      console.error('Ollama è¿æ¥æµ‹è¯•è¯¦æƒ…:', error);

      // æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
      let errorMessage = error.message;

      if (error.name === 'AbortError' || error.name === 'TimeoutError') {
        errorMessage = `è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œåœ¨ ${modelConfig.baseURL}`;
      } else if (error.message === 'Failed to fetch') {
        const currentOrigin = typeof window !== 'undefined' ? window.location.origin : 'unknown';
        errorMessage = `æµè§ˆå™¨æ— æ³•è®¿é—® ${modelConfig.baseURL}

å¯èƒ½åŸå› ï¼šCORS è·¨åŸŸé™åˆ¶

è§£å†³æ–¹æ¡ˆï¼š
1. åœ¨ Ollama æ‰€åœ¨æœºå™¨ï¼ˆWindows/Linuxï¼‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
   OLLAMA_ORIGINS="${currentOrigin}"
   æˆ–
   OLLAMA_ORIGINS="*"  (å…è®¸æ‰€æœ‰æ¥æºï¼Œä»…ç”¨äºå¼€å‘)

2. é‡å¯ Ollama æœåŠ¡

3. éªŒè¯é…ç½®ï¼š
   åœ¨æµè§ˆå™¨æ§åˆ¶å°è¿è¡Œï¼š
   fetch('${modelConfig.baseURL}/v1/models').then(r=>r.json()).then(console.log)

è¯¦ç»†é…ç½®æ–¹æ³•è¯·æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£æˆ– Ollama å®˜æ–¹æ–‡æ¡£ã€‚`;
      }

      throw new APIError(errorMessage);
    }
  }

  /**
   * è·å–æ¨¡å‹åˆ—è¡¨ï¼Œä»¥ä¸‹æ‹‰é€‰é¡¹æ ¼å¼è¿”å›
   * @param provider æä¾›å•†æ ‡è¯†
   * @param customConfig è‡ªå®šä¹‰é…ç½®ï¼ˆå¯é€‰ï¼‰
   */
  async fetchModelList(
    provider: string,
    customConfig?: Partial<ModelConfig>
  ): Promise<ModelOption[]> {
    try {
      // è·å–åŸºç¡€é…ç½®
      let modelConfig = await this.modelManager.getModel(provider);

      // å¦‚æœæä¾›äº†è‡ªå®šä¹‰é…ç½®ï¼Œåˆ™åˆå¹¶åˆ°åŸºç¡€é…ç½®
      if (customConfig) {
        modelConfig = {
          ...modelConfig,
          ...(customConfig as ModelConfig),
        };
      }

      if (!modelConfig) {
        console.warn(`æ¨¡å‹ ${provider} ä¸å­˜åœ¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰é…ç½®`);
        if (!customConfig) {
          throw new RequestConfigError(`æ¨¡å‹ ${provider} ä¸å­˜åœ¨`);
        }
        modelConfig = customConfig as ModelConfig;
      }

      // éªŒè¯å¿…è¦çš„é…ç½®ï¼ˆä»…éªŒè¯API URLï¼‰
      if (!modelConfig.baseURL) {
        throw new RequestConfigError('API URLä¸èƒ½ä¸ºç©º');
      }
      // API keyå…è®¸ä¸ºç©ºå­—ç¬¦ä¸²ï¼ŒæŸäº›æœåŠ¡ï¼ˆå¦‚Ollamaï¼‰ä¸éœ€è¦API key

      let models: ModelInfo[] = [];

      // æ ¹æ®ä¸åŒæä¾›å•†å®ç°ä¸åŒçš„è·å–æ¨¡å‹åˆ—è¡¨é€»è¾‘
      console.log(`è·å– ${modelConfig.name || provider} çš„æ¨¡å‹åˆ—è¡¨`);

      if (provider === 'gemini' || modelConfig.provider === 'gemini') {
        models = await this.fetchGeminiModelsInfo(modelConfig);
      } else if (provider === 'anthropic' || modelConfig.provider === 'anthropic') {
        models = await this.fetchAnthropicModelsInfo(modelConfig);
      } else if (provider === 'deepseek' || modelConfig.provider === 'deepseek') {
        models = await this.fetchDeepSeekModelsInfo(modelConfig);
      } else if (provider === 'ollama' || modelConfig.provider === 'ollama') {
        models = await this.fetchOllamaModelsInfo(modelConfig);
      } else {
        // OpenAIå…¼å®¹æ ¼å¼çš„APIï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰æ¨¡å‹å’ŒOllama
        models = await this.fetchOpenAICompatibleModelsInfo(modelConfig);
      }

      // è½¬æ¢ä¸ºé€‰é¡¹æ ¼å¼
      return models.map(model => ({
        value: model.id,
        label: model.name
      }));
    } catch (error: any) {
      console.error('è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥:', error);
      if (error instanceof RequestConfigError || error instanceof APIError) {
        throw error;
      }
      throw new APIError(`è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * è·å–OpenAIå…¼å®¹APIçš„æ¨¡å‹ä¿¡æ¯
   */
  private async fetchOpenAICompatibleModelsInfo(modelConfig: ModelConfig): Promise<ModelInfo[]> {
    // å…ˆæ£€æŸ¥baseURLæ˜¯å¦ä»¥/v1ç»“å°¾
    if (modelConfig.baseURL && !/\/v1$/.test(modelConfig.baseURL)) {
      throw new APIError(`MISSING_V1_SUFFIX: baseURL should end with "/v1" for OpenAI-compatible APIs. Current: ${modelConfig.baseURL}`);
    }

    const openai = this.getOpenAIInstance(modelConfig);

    try {
      const response = await openai.models.list();
      console.log('APIè¿”å›çš„åŸå§‹æ¨¡å‹åˆ—è¡¨:', response);

      // æ£€æŸ¥è¿”å›æ ¼å¼
      if (response && response.data && Array.isArray(response.data)) {
        const models = response.data
          .map(model => ({
            id: model.id,
            name: model.id
          }))
          .sort((a, b) => a.id.localeCompare(b.id));

        if (models.length === 0) {
          throw new APIError('EMPTY_MODEL_LIST: API returned empty model list');
        }

        return models;
      }

      // è¿”å›æ ¼å¼ä¸å¯¹ï¼ŒæŠ›å‡ºæ ‡å‡†åŒ–é”™è¯¯ä¿¡æ¯
      throw new APIError(`INVALID_RESPONSE_FORMAT: ${JSON.stringify(response)}`);

    } catch (error: any) {
      console.error('Failed to fetch model list:', error);

      // Coreå±‚åªè´Ÿè´£æŠ€æœ¯åˆ¤æ–­ï¼ŒæŠ›å‡ºæ ‡å‡†åŒ–çš„è‹±æ–‡é”™è¯¯ä¿¡æ¯
      if (error.message && (error.message.includes('Failed to fetch') || error.message.includes('Connection error'))) {
        // æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„è·¨åŸŸé”™è¯¯
        // è·¨åŸŸé”™è¯¯çš„ç‰¹å¾ï¼šä¸åŒorigin + æ²¡æœ‰æ˜æ˜¾çš„DNS/è¿æ¥é”™è¯¯
        const errorString = error.toString();
        let isCrossOriginError = false;

        if (modelConfig.baseURL && typeof window !== 'undefined') {
          try {
            const apiUrl = new URL(modelConfig.baseURL);
            const currentUrl = new URL(window.location.href);



            // åªæœ‰åœ¨ä¸åŒoriginä¸”æ²¡æœ‰æ˜æ˜¾çš„DNS/è¿æ¥é”™è¯¯æ—¶æ‰è®¤ä¸ºæ˜¯è·¨åŸŸ
            const isDifferentOrigin = apiUrl.origin !== currentUrl.origin;
            const hasNetworkError = errorString.includes('ERR_NAME_NOT_RESOLVED') ||
                                   errorString.includes('ERR_CONNECTION_REFUSED') ||
                                   errorString.includes('ERR_NETWORK_CHANGED') ||
                                   errorString.includes('ERR_INTERNET_DISCONNECTED') ||
                                   errorString.includes('ERR_EMPTY_RESPONSE');

            isCrossOriginError = isDifferentOrigin && !hasNetworkError;
          } catch (urlError) {
            // URLè§£æå¤±è´¥ï¼Œå½“ä½œæ™®é€šè¿æ¥é”™è¯¯å¤„ç†
          }
        }

        // æ ¹æ®æ£€æµ‹ç»“æœæŠ›å‡ºç›¸åº”é”™è¯¯
        if (isCrossOriginError) {
          throw new APIError(`CROSS_ORIGIN_CONNECTION_FAILED: ${error.message}`);
        } else {
          throw new APIError(`CONNECTION_FAILED: ${error.message}`);
        }
      }

      // APIè¿”å›çš„é”™è¯¯ä¿¡æ¯
      if (error.response?.data) {
        throw new APIError(`API_ERROR: ${JSON.stringify(error.response.data)}`);
      }

      // å…¶ä»–é”™è¯¯ï¼Œä¿æŒåŸå§‹ä¿¡æ¯
      throw new APIError(`UNKNOWN_ERROR: ${error.message || 'Unknown error'}`);
    }
  }
  /**
   * è·å–Geminiæ¨¡å‹ä¿¡æ¯
   */
  private async fetchGeminiModelsInfo(modelConfig: ModelConfig): Promise<ModelInfo[]> {
    console.log(`è·å–${modelConfig.name || 'Gemini'}çš„æ¨¡å‹åˆ—è¡¨`);

    // Gemini APIæ²¡æœ‰ç›´æ¥è·å–æ¨¡å‹åˆ—è¡¨çš„æ¥å£ï¼Œè¿”å›é¢„å®šä¹‰åˆ—è¡¨
    return [
      { id: 'gemini-2.0-flash', name: 'Gemini 2.0 Flash' }
    ];
  }

  /**
   * è·å–Anthropicæ¨¡å‹ä¿¡æ¯
   */
  private async fetchAnthropicModelsInfo(modelConfig: ModelConfig): Promise<ModelInfo[]> {
    console.log(`è·å–${modelConfig.name || 'Anthropic'}çš„æ¨¡å‹åˆ—è¡¨`);

    // Anthropic APIçš„è·å–æ¨¡å‹åˆ—è¡¨åŠŸèƒ½æœªå…¼å®¹openaiæ ¼å¼ï¼Œæ‰€ä»¥è¿™é‡Œè¿”å›ä¸€ä¸ªé»˜è®¤åˆ—è¡¨
    return [
      { id: 'claude-opus-4-20250514', name: 'Claude 4.0 Opus' },
      { id: 'claude-sonnet-4-20250514', name: 'Claude 4.0 Sonnet' },
      { id: 'claude-3-7-sonnet-latest', name: 'Claude 3.7 Sonnet' },
      { id: 'claude-3-5-haiku-latest', name: 'Claude 3.5 Haiku' }
    ];
  }

  /**
   * è·å–DeepSeekæ¨¡å‹ä¿¡æ¯
   */
  private async fetchDeepSeekModelsInfo(modelConfig: ModelConfig): Promise<ModelInfo[]> {
    console.log(`è·å–${modelConfig.name || 'DeepSeek'}çš„æ¨¡å‹åˆ—è¡¨`);

    try {
      // å°è¯•ä½¿ç”¨OpenAIå…¼å®¹APIè·å–æ¨¡å‹åˆ—è¡¨
      return await this.fetchOpenAICompatibleModelsInfo(modelConfig);
    } catch (error) {
      console.error('è·å–DeepSeekæ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨:', error);

      // è¿”å›é»˜è®¤æ¨¡å‹
      return [
        { id: 'deepseek-chat', name: 'DeepSeek Chat' },
        { id: 'deepseek-coder', name: 'DeepSeek Coder' }
      ];
    }
  }

  /**
   * è·å–Ollamaæ¨¡å‹ä¿¡æ¯
   */
  private async fetchOllamaModelsInfo(modelConfig: ModelConfig): Promise<ModelInfo[]> {
    console.log(`è·å–${modelConfig.name || 'Ollama'}çš„æ¨¡å‹åˆ—è¡¨`);

    try {
      // ä¼˜å…ˆä½¿ç”¨ OpenAI å…¼å®¹çš„ /v1/models ç«¯ç‚¹ï¼ˆæ”¯æŒ CORSï¼‰
      const baseURL = modelConfig.baseURL?.replace('/v1', '') || 'http://localhost:11434';

      // é¦–å…ˆå°è¯•ä½¿ç”¨ /v1/models ç«¯ç‚¹ï¼ˆOpenAI å…¼å®¹ï¼Œæœ‰ CORS æ”¯æŒï¼‰
      try {
        const response = await fetch(`${baseURL}/v1/models`);

        if (response.ok) {
          const data = await response.json();

          // OpenAI å…¼å®¹æ ¼å¼: { object: "list", data: [ { id: "model-name", ... }, ... ] }
          if (data && data.data && Array.isArray(data.data)) {
            const models = data.data
              .map((model: any) => ({
                id: model.id,
                name: model.id
              }))
              .sort((a: ModelInfo, b: ModelInfo) => a.id.localeCompare(b.id));

            if (models.length > 0) {
              console.log(`æˆåŠŸè·å– ${models.length} ä¸ª Ollama æ¨¡å‹`);
              return models;
            }
          }
        } else if (response.status === 403) {
          // CORS é”™è¯¯ï¼ŒæŠ›å‡ºè¯¦ç»†è¯´æ˜
          const currentOrigin = typeof window !== 'undefined' ? window.location.origin : 'unknown';
          throw new APIError(`CORS é”™è¯¯: Ollama æ‹’ç»æ¥è‡ª ${currentOrigin} çš„è¯·æ±‚ã€‚è¯·è®¾ç½® OLLAMA_ORIGINS ç¯å¢ƒå˜é‡åé‡å¯ Ollama æœåŠ¡ã€‚`);
        }
      } catch (v1Error: any) {
        // å¦‚æœæ˜¯ CORS é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
        if (v1Error instanceof APIError) {
          throw v1Error;
        }

        // å¦‚æœ /v1/models ä¸å¯ç”¨ï¼Œå°è¯• /api/tagsï¼ˆä»…åœ¨éæµè§ˆå™¨ç¯å¢ƒæˆ–åŒæºæƒ…å†µä¸‹å¯ç”¨ï¼‰
        console.log('/v1/models ç«¯ç‚¹ä¸å¯ç”¨ï¼Œå°è¯• /api/tags ç«¯ç‚¹');

        const tagsResponse = await fetch(`${baseURL}/api/tags`);

        if (!tagsResponse.ok) {
          if (tagsResponse.status === 403) {
            const currentOrigin = typeof window !== 'undefined' ? window.location.origin : 'unknown';
            throw new APIError(`CORS é”™è¯¯: Ollama æ‹’ç»æ¥è‡ª ${currentOrigin} çš„è¯·æ±‚ã€‚è¯·è®¾ç½® OLLAMA_ORIGINS ç¯å¢ƒå˜é‡åé‡å¯ Ollama æœåŠ¡ã€‚`);
          }
          throw new Error(`HTTP error! status: ${tagsResponse.status}`);
        }

        const tagsData = await tagsResponse.json();

        // Ollama åŸç”Ÿæ ¼å¼: { models: [ { name: "llama2:latest", ... }, ... ] }
        if (tagsData && tagsData.models && Array.isArray(tagsData.models)) {
          const models = tagsData.models
            .map((model: any) => ({
              id: model.name,
              name: model.name
            }))
            .sort((a: ModelInfo, b: ModelInfo) => a.id.localeCompare(b.id));

          if (models.length > 0) {
            console.log(`æˆåŠŸè·å– ${models.length} ä¸ª Ollama æ¨¡å‹ï¼ˆé€šè¿‡ /api/tagsï¼‰`);
            return models;
          }
        }
      }

      // å¦‚æœä¸¤ä¸ªç«¯ç‚¹éƒ½æ²¡æœ‰è¿”å›æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›ç©ºåˆ—è¡¨å¹¶ç»™å‡ºè­¦å‘Š
      console.warn('æœªèƒ½ä» Ollama è·å–æ¨¡å‹åˆ—è¡¨ï¼Œè¿”å›ç©ºåˆ—è¡¨');
      return [];

    } catch (error: any) {
      console.error('Failed to fetch Ollama model list:', error);

      // å¦‚æœæ˜¯ APIErrorï¼ˆCORS é”™è¯¯ï¼‰ï¼Œç›´æ¥æŠ›å‡º
      if (error instanceof APIError) {
        throw error;
      }

      // å…¶ä»–é”™è¯¯ï¼ŒåŒ…è£…åæŠ›å‡º
      throw new APIError(`è·å– Ollama æ¨¡å‹åˆ—è¡¨å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * æ„å»ºGeminiç”Ÿæˆé…ç½®
   * 
   * æ³¨æ„ï¼šæ­¤æ–¹æ³•å‡è®¾ä¼ å…¥çš„ llmParams å·²ç»é€šè¿‡ ModelManager.validateConfig() 
   * ä¸­çš„ validateLLMParams éªŒè¯ï¼Œç¡®ä¿å®‰å…¨æ€§
   */
  private buildGeminiGenerationConfig(llmParams: Record<string, any> = {}): any {
    const {
      temperature,
      maxOutputTokens,
      topP,
      topK,
      candidateCount,
      stopSequences,
      ...otherParams
    } = llmParams;

    const generationConfig: any = {};
    
    // æ·»åŠ å·²çŸ¥å‚æ•°
    if (temperature !== undefined) {
      generationConfig.temperature = temperature;
    }
    if (maxOutputTokens !== undefined) {
      generationConfig.maxOutputTokens = maxOutputTokens;
    }
    if (topP !== undefined) {
      generationConfig.topP = topP;
    }
    if (topK !== undefined) {
      generationConfig.topK = topK;
    }
    if (candidateCount !== undefined) {
      generationConfig.candidateCount = candidateCount;
    }
    if (stopSequences !== undefined && Array.isArray(stopSequences)) {
      generationConfig.stopSequences = stopSequences;
    }

    // æ·»åŠ å…¶ä»–å‚æ•° (å·²åœ¨ä¸Šå±‚éªŒè¯è¿‡å®‰å…¨æ€§)
    // æ’é™¤ä¸€äº›æ˜æ˜¾ä¸å±äº Gemini generationConfig çš„å‚æ•°
    for (const [key, value] of Object.entries(otherParams)) {
      if (!['timeout', 'model', 'messages', 'stream'].includes(key)) {
        generationConfig[key] = value;
      }
    }

    return generationConfig;
  }
}

/**
 * åˆ›å»ºLLMæœåŠ¡å®ä¾‹çš„å·¥å‚å‡½æ•°
 * @param modelManager æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
 * @returns LLMæœåŠ¡å®ä¾‹
 */
export function createLLMService(modelManager: ModelManager): ILLMService {
  // åœ¨Electronç¯å¢ƒä¸­ï¼Œè¿”å›ä»£ç†å®ä¾‹
  if (isRunningInElectron()) {
    console.log('[LLM Service Factory] Electron environment detected, using proxy.');
    return new ElectronLLMProxy();
  }
  return new LLMService(modelManager);
} 
